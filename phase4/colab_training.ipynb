{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743535d6",
   "metadata": {},
   "source": [
    "# üß† Phase 4: Subvocalization Classification\n",
    "**Single-Channel sEMG ‚Üí Silent Speech Recognition**\n",
    "\n",
    "Transfer Learning: Mouthing (L3) ‚Üí Subvocal (L4)\n",
    "\n",
    "---\n",
    "**Author:** Carl Kho | **Date:** December 2025 | **GPU:** A100 Recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f3b59",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn tensorflow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5860ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your data.zip (contains CSV files)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Upload your speech-capture.zip file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract\n",
    "zip_name = list(uploaded.keys())[0]\n",
    "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')\n",
    "\n",
    "print(\"\\n‚úÖ Extracted files:\")\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(1738)\n",
    "tf.random.set_seed(1738)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04ec58",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb5977",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Find the data directory (handles nested extraction)\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob('data/**/*.csv', recursive=True)\n",
    "if not csv_files:\n",
    "    csv_files = glob.glob('data/*.csv')\n",
    "\n",
    "DATA_DIR = os.path.dirname(csv_files[0]) if csv_files else 'data'\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Found CSVs: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrum_data(data_dir):\n",
    "    \"\"\"Load all motor intensity spectrum CSV files.\"\"\"\n",
    "    files = {\n",
    "        'overt': 'overt_data.csv',\n",
    "        'whisper': 'whisper_data.csv',\n",
    "        'mouthing': 'mouthing_data.csv',\n",
    "        'subvocal': 'subvocal_data.csv',\n",
    "        'imagined': 'imagined_data.csv'\n",
    "    }\n",
    "\n",
    "    data = {}\n",
    "    for level, filename in files.items():\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            data[level] = df\n",
    "            print(f\"‚úÖ Loaded {level}: {len(df):,} samples\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Missing: {filename}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_spectrum_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "print(\"\\nüìä Sample data (mouthing):\")\n",
    "data['mouthing'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb356b6",
   "metadata": {},
   "source": [
    "## üìã Due Diligence: Data Quality Assessment\n",
    "\n",
    "Before modeling, we perform comprehensive diagnostics to verify data quality,\n",
    "class balance, and signal characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62dcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìã DUE DILIGENCE: Comprehensive Diagnostics\n",
    "# ==========================================\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã DUE DILIGENCE: Data Quality Assessment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Class Balance Check\n",
    "print(\"\\n1Ô∏è‚É£ CLASS BALANCE:\")\n",
    "for level_name, df in data.items():\n",
    "    print(f\"\\n  {level_name.upper()}:\")\n",
    "    class_counts = df['Label'].value_counts()\n",
    "    total = len(df)\n",
    "    for label, count in class_counts.items():\n",
    "        pct = count / total * 100\n",
    "        print(f\"    {label}: {count:,} samples ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae65486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Signal Statistics Comparison\n",
    "print(\"\\n2Ô∏è‚É£ SIGNAL STATISTICS (Raw ADC values):\")\n",
    "stats_data = []\n",
    "for level_name, df in data.items():\n",
    "    stats = {\n",
    "        'Level': level_name.upper(),\n",
    "        'Mean': df['RawValue'].mean(),\n",
    "        'Std': df['RawValue'].std(),\n",
    "        'Min': df['RawValue'].min(),\n",
    "        'Max': df['RawValue'].max(),\n",
    "        'Range': df['RawValue'].max() - df['RawValue'].min()\n",
    "    }\n",
    "    stats_data.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a167cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Amplitude Comparison Across Levels (Visual)\n",
    "print(\"\\n3Ô∏è‚É£ AMPLITUDE COMPARISON ACROSS MOTOR INTENSITY LEVELS:\")\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharey=True)\n",
    "levels = ['overt', 'whisper', 'mouthing', 'subvocal', 'imagined']\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#95a5a6']\n",
    "\n",
    "for ax, level, color in zip(axes, levels, colors):\n",
    "    if level in data:\n",
    "        # Take a 3-second sample\n",
    "        sample = data[level]['RawValue'].values[:3000]\n",
    "        ax.plot(sample, linewidth=0.5, color=color)\n",
    "        ax.set_title(f'{level.upper()}\\n(n={len(data[level]):,})', fontweight='bold')\n",
    "        ax.set_xlabel('Samples')\n",
    "\n",
    "axes[0].set_ylabel('Raw ADC Value')\n",
    "plt.suptitle('Signal Amplitude Across Motor Intensity Levels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_amplitude_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc839a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sample Duration Distribution\n",
    "print(\"\\n4Ô∏è‚É£ SAMPLE DURATION PER WORD (Block Lengths):\")\n",
    "for level_name in ['mouthing', 'subvocal']:\n",
    "    if level_name in data:\n",
    "        df = data[level_name].copy()\n",
    "        df['label_change'] = df['Label'] != df['Label'].shift(1)\n",
    "        df['block_id'] = df['label_change'].cumsum()\n",
    "        block_lengths = df.groupby('block_id').size()\n",
    "\n",
    "        print(f\"\\n  {level_name.upper()}:\")\n",
    "        print(f\"    Mean block length: {block_lengths.mean():.0f} samples ({block_lengths.mean()/1000:.2f}s)\")\n",
    "        print(f\"    Std: {block_lengths.std():.0f} samples\")\n",
    "        print(f\"    Min: {block_lengths.min()} | Max: {block_lengths.max()}\")\n",
    "        print(f\"    Total blocks: {len(block_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ADC Value Distribution\n",
    "print(\"\\n5Ô∏è‚É£ ADC VALUE DISTRIBUTION (Histogram):\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mouthing\n",
    "axes[0].hist(data['mouthing']['RawValue'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Mouthing (L3) ADC Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('ADC Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(data['mouthing']['RawValue'].mean(), color='red', linestyle='--', label=f\"Mean: {data['mouthing']['RawValue'].mean():.0f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Subvocal\n",
    "axes[1].hist(data['subvocal']['RawValue'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Subvocal (L4) ADC Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('ADC Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(data['subvocal']['RawValue'].mean(), color='red', linestyle='--', label=f\"Mean: {data['subvocal']['RawValue'].mean():.0f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_adc_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aede60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Per-Class Signal Comparison (Mouthing)\n",
    "print(\"\\n6Ô∏è‚É£ PER-CLASS STATISTICS (Mouthing):\")\n",
    "for label in data['mouthing']['Label'].unique():\n",
    "    subset = data['mouthing'][data['mouthing']['Label'] == label]['RawValue']\n",
    "    print(f\"  {label}: mean={subset.mean():.1f}, std={subset.std():.1f}, range=[{subset.min()}, {subset.max()}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DUE DILIGENCE COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3307b23",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Preprocessing & Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ebec0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "def bandpass_filter(signal, fs=1000, lowcut=1.0, highcut=45.0, order=4):\n",
    "    \"\"\"Apply Butterworth bandpass filter.\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def notch_filter(signal, fs=1000, freq=60.0, Q=30.0):\n",
    "    \"\"\"Remove 60Hz power line noise.\"\"\"\n",
    "    b, a = iirnotch(freq, Q, fs)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def preprocess_signal(signal):\n",
    "    \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "    signal = bandpass_filter(signal)\n",
    "    signal = notch_filter(signal)\n",
    "    # Z-score normalize\n",
    "    return (signal - signal.mean()) / (signal.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df, window_size=1000, center_offset=1000):\n",
    "    \"\"\"\n",
    "    Create fixed-size windows grouped by label transitions.\n",
    "\n",
    "    IMPORTANT: Words were vocalized at countdown \"2\" (middle of 3-sec window).\n",
    "    So we extract samples [1000:2000] to capture the actual articulation.\n",
    "\n",
    "    Args:\n",
    "        window_size: Size of window to extract (1000 = 1 second @ 1000Hz)\n",
    "        center_offset: Where the word starts in the raw block (1000 = at 1 second)\n",
    "    \"\"\"\n",
    "    windows_X = []\n",
    "    windows_y = []\n",
    "\n",
    "    # Find label transitions\n",
    "    df['label_change'] = df['Label'] != df['Label'].shift(1)\n",
    "    df['block_id'] = df['label_change'].cumsum()\n",
    "\n",
    "    for block_id, block in df.groupby('block_id'):\n",
    "        label = block['Label'].iloc[0]\n",
    "        signal = block['RawValue'].values\n",
    "\n",
    "        # Extract MIDDLE portion where word was actually spoken\n",
    "        # Words vocalized at countdown \"2\" = samples 1000-2000\n",
    "        if len(signal) >= center_offset + window_size:\n",
    "            signal = signal[center_offset:center_offset + window_size]\n",
    "        elif len(signal) >= window_size:\n",
    "            # Fallback: take last window_size samples\n",
    "            signal = signal[-window_size:]\n",
    "        else:\n",
    "            # Pad if too short\n",
    "            pad_size = window_size - len(signal)\n",
    "            signal = np.pad(signal, (0, pad_size), mode='mean')\n",
    "\n",
    "        # Preprocess\n",
    "        try:\n",
    "            signal = preprocess_signal(signal)\n",
    "            windows_X.append(signal.reshape(-1, 1))\n",
    "            windows_y.append(label)\n",
    "        except:\n",
    "            continue  # Skip problematic windows\n",
    "\n",
    "    return np.array(windows_X), np.array(windows_y)\n",
    "\n",
    "print(\"Creating windows (extracting MIDDLE 1-second where word was spoken)...\")\n",
    "X_mouthing, y_mouthing = create_windows(data['mouthing'])\n",
    "X_subvocal, y_subvocal = create_windows(data['subvocal'])\n",
    "\n",
    "print(f\"\\nüìä Mouthing (L3 - Training): {X_mouthing.shape}\")\n",
    "print(f\"üìä Subvocal (L4 - Testing): {X_subvocal.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize random samples for each class\n",
    "print(\"\\nüìä Random samples per class (Mouthing - L3):\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    # Find indices for this class\n",
    "    cls_indices = np.where(y_mouthing == cls)[0]\n",
    "    if len(cls_indices) > 0:\n",
    "        # Pick random sample\n",
    "        rand_idx = np.random.choice(cls_indices)\n",
    "        signal = X_mouthing[rand_idx].flatten()\n",
    "\n",
    "        axes[i].plot(signal, linewidth=0.8, color='steelblue')\n",
    "        axes[i].set_title(f'{cls} (sample #{rand_idx})', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Samples (1000 = 1 second)')\n",
    "        axes[i].set_ylabel('Normalized Amplitude')\n",
    "        axes[i].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Random Signal Samples per Word Class (Mouthing - L3)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_random_samples_mouthing.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Same for subvocal\n",
    "print(\"\\nüìä Random samples per class (Subvocal - L4):\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    cls_indices = np.where(y_subvocal == cls)[0]\n",
    "    if len(cls_indices) > 0:\n",
    "        rand_idx = np.random.choice(cls_indices)\n",
    "        signal = X_subvocal[rand_idx].flatten()\n",
    "\n",
    "        axes[i].plot(signal, linewidth=0.8, color='coral')\n",
    "        axes[i].set_title(f'{cls} (sample #{rand_idx})', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Samples (1000 = 1 second)')\n",
    "        axes[i].set_ylabel('Normalized Amplitude')\n",
    "        axes[i].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Random Signal Samples per Word Class (Subvocal - L4)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_random_samples_subvocal.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(np.concatenate([y_mouthing, y_subvocal]))\n",
    "\n",
    "y_mouthing_enc = le.transform(y_mouthing)\n",
    "y_subvocal_enc = le.transform(y_subvocal)\n",
    "\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Label mapping: {dict(zip(le.classes_, range(len(le.classes_))))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddef847",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train/Val split on mouthing (source domain)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_mouthing, y_mouthing_enc,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_mouthing_enc\n",
    ")\n",
    "\n",
    "# Test set is subvocal (target domain)\n",
    "X_test, y_test = X_subvocal, y_subvocal_enc\n",
    "\n",
    "print(f\"\\nüìä Train: {X_train.shape}\")\n",
    "print(f\"üìä Val: {X_val.shape}\")\n",
    "print(f\"üìä Test (L4): {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48552fa5",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Feature Extraction (EXTENDED for better accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01d327",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ACCURACY IMPROVEMENT #1: Extended Feature Set\n",
    "def extract_features_extended(X):\n",
    "    \"\"\"\n",
    "    Extract EXTENDED features for better discrimination.\n",
    "\n",
    "    Features:\n",
    "    - Time domain: MAV, ZCR, SD, MAX, RMS, Waveform Length\n",
    "    - Temporal: Energy in 4 quarters\n",
    "    - Frequency: Dominant frequency, spectral centroid\n",
    "\n",
    "    Total: 14 features per window\n",
    "    \"\"\"\n",
    "    from scipy.fft import fft\n",
    "\n",
    "    features = []\n",
    "    for window in X:\n",
    "        signal = window.flatten()\n",
    "        n = len(signal)\n",
    "\n",
    "        # Time domain features\n",
    "        mav = np.mean(np.abs(signal))\n",
    "        zcr = np.sum(np.diff(np.sign(signal)) != 0)\n",
    "        sd = np.std(signal)\n",
    "        max_amp = np.max(np.abs(signal))\n",
    "        rms = np.sqrt(np.mean(signal**2))\n",
    "        waveform_length = np.sum(np.abs(np.diff(signal)))\n",
    "\n",
    "        # Temporal features (quarters)\n",
    "        e1 = np.mean(np.abs(signal[:n//4]))\n",
    "        e2 = np.mean(np.abs(signal[n//4:n//2]))\n",
    "        e3 = np.mean(np.abs(signal[n//2:3*n//4]))\n",
    "        e4 = np.mean(np.abs(signal[3*n//4:]))\n",
    "\n",
    "        # Frequency features (FFT)\n",
    "        fft_vals = np.abs(fft(signal))[:n//2]  # Only positive frequencies\n",
    "        freqs = np.linspace(0, 500, n//2)  # 0-500Hz for 1000Hz sampling\n",
    "\n",
    "        # Dominant frequency\n",
    "        dom_freq_idx = np.argmax(fft_vals[1:]) + 1  # Skip DC\n",
    "        dom_freq = freqs[dom_freq_idx]\n",
    "\n",
    "        # Spectral centroid\n",
    "        spectral_centroid = np.sum(freqs * fft_vals) / (np.sum(fft_vals) + 1e-8)\n",
    "\n",
    "        # Spectral energy in speech band (1-45Hz)\n",
    "        speech_band_mask = (freqs >= 1) & (freqs <= 45)\n",
    "        speech_band_energy = np.sum(fft_vals[speech_band_mask])\n",
    "\n",
    "        features.append([\n",
    "            mav, zcr, sd, max_amp, rms, waveform_length,\n",
    "            e1, e2, e3, e4,\n",
    "            dom_freq, spectral_centroid, speech_band_energy,\n",
    "            e2 - e1  # Onset indicator\n",
    "        ])\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Extracting EXTENDED features (14 features)...\")\n",
    "X_train_feat = extract_features_extended(X_train)\n",
    "X_val_feat = extract_features_extended(X_val)\n",
    "X_test_feat = extract_features_extended(X_test)\n",
    "\n",
    "print(f\"Feature shapes: Train {X_train_feat.shape}, Val {X_val_feat.shape}, Test {X_test_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY IMPROVEMENT #2: Data Augmentation\n",
    "def augment_data(X, y, factor=3):\n",
    "    \"\"\"\n",
    "    Augment training data with jitter, scaling, and time shift.\n",
    "    \"\"\"\n",
    "    X_aug = [X]\n",
    "    y_aug = [y]\n",
    "\n",
    "    for _ in range(factor - 1):\n",
    "        X_new = []\n",
    "        for window in X:\n",
    "            aug = window.copy()\n",
    "\n",
    "            # Random jitter\n",
    "            aug += np.random.normal(0, 0.05, aug.shape)\n",
    "\n",
    "            # Random scaling\n",
    "            aug *= np.random.uniform(0.9, 1.1)\n",
    "\n",
    "            # Random time shift (circular)\n",
    "            shift = np.random.randint(-50, 50)\n",
    "            aug = np.roll(aug, shift, axis=0)\n",
    "\n",
    "            X_new.append(aug)\n",
    "\n",
    "        X_aug.append(np.array(X_new))\n",
    "        y_aug.append(y)\n",
    "\n",
    "    return np.vstack(X_aug), np.hstack(y_aug)\n",
    "\n",
    "print(\"\\nAugmenting training data (3x)...\")\n",
    "X_train_aug, y_train_aug = augment_data(X_train, y_train, factor=3)\n",
    "X_train_feat_aug = extract_features_extended(X_train_aug)\n",
    "print(f\"Augmented: {X_train_feat.shape} ‚Üí {X_train_feat_aug.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9866c0",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå≤ Training Random Forest (with AUGMENTED data)...\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  # More trees for better accuracy\n",
    "    max_depth=20,      # Prevent overfitting\n",
    "    min_samples_split=5,\n",
    "    random_state=1738,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train on AUGMENTED data\n",
    "rf.fit(X_train_feat_aug, y_train_aug)\n",
    "\n",
    "# Evaluate on source domain (val)\n",
    "y_val_pred = rf.predict(X_val_feat)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Evaluate on target domain (test = L4)\n",
    "y_test_pred = rf.predict(X_test_feat)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Val Accuracy (L3): {val_acc:.4f}\")\n",
    "print(f\"‚úÖ Test Accuracy (L4): {test_acc:.4f}\")\n",
    "print(f\"üìâ Transfer Gap: {val_acc - test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Would non-augmented do better?\n",
    "print(\"\\nüìä Ablation: Augmented vs Non-Augmented:\")\n",
    "rf_no_aug = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=1738, n_jobs=-1)\n",
    "rf_no_aug.fit(X_train_feat, y_train)\n",
    "no_aug_acc = accuracy_score(y_test, rf_no_aug.predict(X_test_feat))\n",
    "print(f\"  Without augmentation: {no_aug_acc:.4f}\")\n",
    "print(f\"  With augmentation:    {test_acc:.4f}\")\n",
    "print(f\"  Improvement: {(test_acc - no_aug_acc)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nüìä Classification Report (Test - L4):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'Random Forest Confusion Matrix (Test Acc: {test_acc:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c328cf",
   "metadata": {},
   "source": [
    "## üî¨ SANITY CHECK: Same-Domain Classification\n",
    "\n",
    "Before attempting cross-domain transfer (L3‚ÜíL4), let's verify that we can\n",
    "classify words **within the same domain**. This tells us: \"Is the signal\n",
    "even classifiable with our hardware?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a83377",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üî¨ SANITY CHECK: Same-Domain Classification\n",
    "# ==========================================\n",
    "print(\"=\" * 60)\n",
    "print(\"üî¨ SANITY CHECK: Train & Test on MOUTHING ONLY (same domain)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split mouthing data properly (train/test from same source)\n",
    "X_m_train, X_m_test, y_m_train, y_m_test = train_test_split(\n",
    "    X_mouthing, y_mouthing_enc,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y_mouthing_enc\n",
    ")\n",
    "\n",
    "# Extract features\n",
    "X_m_train_feat = extract_features(X_m_train)\n",
    "X_m_test_feat = extract_features(X_m_test)\n",
    "\n",
    "# Train RF on mouthing only\n",
    "rf_sanity = RandomForestClassifier(n_estimators=100, random_state=1738, n_jobs=-1)\n",
    "rf_sanity.fit(X_m_train_feat, y_m_train)\n",
    "\n",
    "# Evaluate same-domain\n",
    "y_m_pred = rf_sanity.predict(X_m_test_feat)\n",
    "sanity_acc = accuracy_score(y_m_test, y_m_pred)\n",
    "\n",
    "print(f\"\\n‚úÖ Same-Domain Accuracy (L3‚ÜíL3): {sanity_acc:.4f}\")\n",
    "print(f\"   (This is the 'ceiling' - best we can do)\")\n",
    "print(f\"\\nüìä Classification Report (L3 only):\")\n",
    "print(classification_report(y_m_test, y_m_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_m_test, y_m_pred, normalize='true')\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Purples',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'SANITY CHECK: Mouthing Only (Acc: {sanity_acc:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sanity_check_mouthing.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"  > 70%: Signal is good! Transfer L3‚ÜíL4 is the hard part\")\n",
    "print(\"  < 50%: Hardware/signal issue - words may not be distinguishable\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859c200",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ MaxCRNN (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block(x, filters):\n",
    "    \"\"\"1D Inception block with parallel convolutions.\"\"\"\n",
    "    conv1 = layers.Conv1D(filters, 1, padding='same', activation='relu')(x)\n",
    "    conv3 = layers.Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
    "    conv5 = layers.Conv1D(filters, 5, padding='same', activation='relu')(x)\n",
    "    pool = layers.MaxPooling1D(3, strides=1, padding='same')(x)\n",
    "    pool = layers.Conv1D(filters, 1, padding='same', activation='relu')(pool)\n",
    "    return layers.Concatenate()([conv1, conv3, conv5, pool])\n",
    "\n",
    "def build_maxcrnn(input_shape, n_classes):\n",
    "    \"\"\"Build MaxCRNN: Inception + Bi-LSTM + Attention\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Inception blocks\n",
    "    x = inception_block(inputs, filters=32)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = inception_block(x, filters=64)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    # Bi-LSTM\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    x = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
    "\n",
    "    # Classification head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs, name='MaxCRNN')\n",
    "\n",
    "# Build model\n",
    "n_classes = len(le.classes_)\n",
    "model = build_maxcrnn(input_shape=(X_train.shape[1], 1), n_classes=n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a172d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "print(\"üöÄ Training MaxCRNN...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('maxcrnn_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54104b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MaxCRNN\n",
    "print(\"\\nüìä MaxCRNN Evaluation:\")\n",
    "\n",
    "# Val (L3)\n",
    "val_loss, val_acc_nn = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Val Accuracy (L3): {val_acc_nn:.4f}\")\n",
    "\n",
    "# Test (L4)\n",
    "test_loss, test_acc_nn = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy (L4): {test_acc_nn:.4f}\")\n",
    "print(f\"Transfer Gap: {val_acc_nn - test_acc_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxCRNN Confusion Matrix\n",
    "y_test_pred_nn = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_test_pred_nn, normalize='true')\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Greens',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'MaxCRNN Confusion Matrix (Test Acc: {test_acc_nn:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('maxcrnn_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Classification Report (MaxCRNN - L4):\")\n",
    "print(classification_report(y_test, y_test_pred_nn, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cb2df",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "rf_test_acc = accuracy_score(y_test, y_test_pred)\n",
    "nn_test_acc = accuracy_score(y_test, y_test_pred_nn)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'MaxCRNN'],\n",
    "    'Test Accuracy (L4)': [rf_test_acc, nn_test_acc],\n",
    "    'Val Accuracy (L3)': [accuracy_score(y_val, rf.predict(X_val_feat)), val_acc_nn]\n",
    "})\n",
    "\n",
    "print(\"\\nüèÜ Model Comparison:\")\n",
    "print(results)\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, results['Val Accuracy (L3)'], width, label='Val (L3)', color='steelblue')\n",
    "plt.bar(x + width/2, results['Test Accuracy (L4)'], width, label='Test (L4)', color='coral')\n",
    "plt.xticks(x, results['Model'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Transfer Learning: L3 ‚Üí L4')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(results['Test Accuracy (L4)']):\n",
    "    plt.text(i + width/2, v + 0.02, f'{v:.2%}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c3095",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Models & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Random Forest\n",
    "import pickle\n",
    "with open('random_forest_phase4.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "print(\"‚úÖ Saved: random_forest_phase4.pkl\")\n",
    "\n",
    "# Save MaxCRNN\n",
    "model.save('maxcrnn_phase4.keras')\n",
    "print(\"‚úÖ Saved: maxcrnn_phase4.keras\")\n",
    "\n",
    "# Save label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "print(\"‚úÖ Saved: label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "\n",
    "# Zip everything\n",
    "!zip -r phase4_results.zip *.pkl *.keras *.png\n",
    "\n",
    "files.download('phase4_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b2dde",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "| Model | Val Acc (L3) | Test Acc (L4) | Transfer Gap |\n",
    "|-------|--------------|---------------|--------------|\n",
    "| Random Forest | TBD | TBD | TBD |\n",
    "| MaxCRNN | TBD | TBD | TBD |\n",
    "\n",
    "---\n",
    "**Next Steps:**\n",
    "1. Fill TBD values after running\n",
    "2. Export confusion matrices for assignment\n",
    "3. Analyze per-class performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a11bb4",
   "metadata": {},
   "source": [
    "---\n",
    "# üöÄ ADVANCED STRATEGIES FOR ACCURACY IMPROVEMENT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146848ee",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Binary Classification: WORD vs REST\n",
    "\n",
    "Before attempting 4-class classification, let's see if we can at least\n",
    "distinguish \"any word\" from \"rest\". This is often easier and can give us\n",
    "confidence in the signal quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üéØ STRATEGY 1: Binary Classification (WORD vs REST)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to binary labels\n",
    "def to_binary(y, le):\n",
    "    \"\"\"Convert multi-class labels to binary (WORD vs REST).\"\"\"\n",
    "    class_names = le.classes_\n",
    "    rest_idx = np.where(class_names == 'REST')[0][0] if 'REST' in class_names else -1\n",
    "    return (y != rest_idx).astype(int)  # 0=REST, 1=WORD\n",
    "\n",
    "y_train_binary = to_binary(y_train, le)\n",
    "y_val_binary = to_binary(y_val, le)\n",
    "y_test_binary = to_binary(y_test, le)\n",
    "\n",
    "print(f\"Binary distribution (train): REST={np.sum(y_train_binary==0)}, WORD={np.sum(y_train_binary==1)}\")\n",
    "\n",
    "# Train binary RF\n",
    "rf_binary = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=1738, n_jobs=-1)\n",
    "rf_binary.fit(X_train_feat_aug, np.tile(y_train_binary, 3))  # Match augmented size\n",
    "\n",
    "# Evaluate\n",
    "y_test_pred_binary = rf_binary.predict(X_test_feat)\n",
    "binary_acc = accuracy_score(y_test_binary, y_test_pred_binary)\n",
    "\n",
    "print(f\"\\n‚úÖ Binary Accuracy (WORD vs REST): {binary_acc:.4f}\")\n",
    "print(f\"   (If this is high, signal is good but words are hard to distinguish)\")\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm_binary = confusion_matrix(y_test_binary, y_test_pred_binary, normalize='true')\n",
    "sns.heatmap(cm_binary, annot=True, fmt='.2f', cmap='Oranges',\n",
    "            xticklabels=['REST', 'WORD'], yticklabels=['REST', 'WORD'])\n",
    "plt.title(f'Binary Classification (Acc: {binary_acc:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('binary_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27d07a",
   "metadata": {},
   "source": [
    "## üîü Spectrogram + 2D CNN (ImageNet Transfer Learning)\n",
    "\n",
    "Convert 1D signals to mel-spectrograms and use MobileNetV2 pretrained on ImageNet.\n",
    "This leverages millions of image-trained weights for pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874697e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üéØ STRATEGY 2: Spectrogram + MobileNetV2 Transfer Learning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def signal_to_spectrogram(signal, sr=1000, n_mels=64, target_size=(96, 96)):\n",
    "    \"\"\"\n",
    "    Convert 1D EMG signal to mel-spectrogram image.\n",
    "    \"\"\"\n",
    "    # Compute mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=signal.astype(float),\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        fmax=sr/2\n",
    "    )\n",
    "    # Convert to dB\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Normalize to 0-255\n",
    "    S_norm = ((S_dB - S_dB.min()) / (S_dB.max() - S_dB.min() + 1e-8) * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize to target size\n",
    "    from scipy.ndimage import zoom\n",
    "    zoom_factors = (target_size[0] / S_norm.shape[0], target_size[1] / S_norm.shape[1])\n",
    "    S_resized = zoom(S_norm, zoom_factors, order=1)\n",
    "\n",
    "    # Convert to 3-channel (RGB) for ImageNet models\n",
    "    S_rgb = np.stack([S_resized, S_resized, S_resized], axis=-1)\n",
    "\n",
    "    return S_rgb\n",
    "\n",
    "# Create spectrogram dataset\n",
    "print(\"\\nConverting signals to spectrograms...\")\n",
    "X_train_spec = np.array([signal_to_spectrogram(x.flatten()) for x in X_train])\n",
    "X_val_spec = np.array([signal_to_spectrogram(x.flatten()) for x in X_val])\n",
    "X_test_spec = np.array([signal_to_spectrogram(x.flatten()) for x in X_test])\n",
    "\n",
    "print(f\"Spectrogram shapes: Train {X_train_spec.shape}, Val {X_val_spec.shape}, Test {X_test_spec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d94846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample spectrograms\n",
    "print(\"\\nüìä Sample spectrograms per class:\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    cls_idx = np.where(le.classes_ == cls)[0][0]\n",
    "    sample_indices = np.where(y_train == cls_idx)[0]\n",
    "    if len(sample_indices) > 0:\n",
    "        idx = sample_indices[0]\n",
    "        axes[i].imshow(X_train_spec[idx], aspect='auto', origin='lower')\n",
    "        axes[i].set_title(f'{cls}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Time')\n",
    "        axes[i].set_ylabel('Mel Bins')\n",
    "\n",
    "plt.suptitle('Mel-Spectrograms per Word Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz_spectrograms.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MobileNetV2 transfer learning model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "def build_spectrogram_cnn(input_shape=(96, 96, 3), n_classes=4):\n",
    "    \"\"\"\n",
    "    MobileNetV2 with ImageNet weights for spectrogram classification.\n",
    "    \"\"\"\n",
    "    # Load pretrained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze base layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(base_model.input, outputs, name='SpecCNN_MobileNetV2')\n",
    "    return model\n",
    "\n",
    "print(\"\\nBuilding MobileNetV2 spectrogram model...\")\n",
    "spec_model = build_spectrogram_cnn(n_classes=len(le.classes_))\n",
    "spec_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(f\"Total params: {spec_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train spectrogram model\n",
    "print(\"\\nüöÄ Training Spectrogram CNN...\")\n",
    "spec_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Normalize spectrograms for ImageNet\n",
    "X_train_spec_norm = X_train_spec / 255.0\n",
    "X_val_spec_norm = X_val_spec / 255.0\n",
    "X_test_spec_norm = X_test_spec / 255.0\n",
    "\n",
    "spec_history = spec_model.fit(\n",
    "    X_train_spec_norm, y_train,\n",
    "    validation_data=(X_val_spec_norm, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=spec_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fedab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate spectrogram model\n",
    "print(\"\\nüìä Spectrogram CNN Evaluation:\")\n",
    "val_loss, val_acc_spec = spec_model.evaluate(X_val_spec_norm, y_val, verbose=0)\n",
    "test_loss, test_acc_spec = spec_model.evaluate(X_test_spec_norm, y_test, verbose=0)\n",
    "\n",
    "print(f\"Val Accuracy (L3): {val_acc_spec:.4f}\")\n",
    "print(f\"Test Accuracy (L4): {test_acc_spec:.4f}\")\n",
    "print(f\"Transfer Gap: {val_acc_spec - test_acc_spec:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_test_pred_spec = np.argmax(spec_model.predict(X_test_spec_norm), axis=1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_spec = confusion_matrix(y_test, y_test_pred_spec, normalize='true')\n",
    "sns.heatmap(cm_spec, annot=True, fmt='.2f', cmap='Reds',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'Spectrogram CNN Confusion Matrix (Acc: {test_acc_spec:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('spectrogram_cnn_confusion.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a96a9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Window Overlap Strategy\n",
    "\n",
    "Instead of non-overlapping windows, use 50% overlap to create more training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üéØ STRATEGY 3: Window Overlap (50%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_windows_overlap(df, window_size=1000, overlap=0.5, center_offset=500):\n",
    "    \"\"\"\n",
    "    Create overlapping windows for more training data.\n",
    "\n",
    "    Args:\n",
    "        window_size: Window size in samples\n",
    "        overlap: Overlap fraction (0.5 = 50%)\n",
    "        center_offset: Where word starts in block\n",
    "    \"\"\"\n",
    "    step = int(window_size * (1 - overlap))\n",
    "    windows_X = []\n",
    "    windows_y = []\n",
    "\n",
    "    df['label_change'] = df['Label'] != df['Label'].shift(1)\n",
    "    df['block_id'] = df['label_change'].cumsum()\n",
    "\n",
    "    for block_id, block in df.groupby('block_id'):\n",
    "        label = block['Label'].iloc[0]\n",
    "        signal = block['RawValue'].values\n",
    "\n",
    "        # Slide window with overlap\n",
    "        for start in range(0, max(1, len(signal) - window_size), step):\n",
    "            window = signal[start:start + window_size]\n",
    "            if len(window) == window_size:\n",
    "                try:\n",
    "                    window = preprocess_signal(window)\n",
    "                    windows_X.append(window.reshape(-1, 1))\n",
    "                    windows_y.append(label)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return np.array(windows_X), np.array(windows_y)\n",
    "\n",
    "# Create overlapped windows\n",
    "X_mouth_overlap, y_mouth_overlap = create_windows_overlap(data['mouthing'].copy())\n",
    "y_mouth_overlap_enc = le.transform(y_mouth_overlap)\n",
    "\n",
    "print(f\"Original windows: {X_mouthing.shape[0]}\")\n",
    "print(f\"Overlapped windows: {X_mouth_overlap.shape[0]} ({X_mouth_overlap.shape[0]/X_mouthing.shape[0]:.1f}x more)\")\n",
    "\n",
    "# Train/test split on overlapped data\n",
    "X_train_ov, X_val_ov, y_train_ov, y_val_ov = train_test_split(\n",
    "    X_mouth_overlap, y_mouth_overlap_enc, test_size=0.15, random_state=42, stratify=y_mouth_overlap_enc\n",
    ")\n",
    "\n",
    "# Extract features\n",
    "X_train_ov_feat = extract_features_extended(X_train_ov)\n",
    "X_val_ov_feat = extract_features_extended(X_val_ov)\n",
    "\n",
    "# Train RF on overlapped data\n",
    "rf_overlap = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=1738, n_jobs=-1)\n",
    "rf_overlap.fit(X_train_ov_feat, y_train_ov)\n",
    "\n",
    "# Evaluate on original test set\n",
    "y_test_pred_ov = rf_overlap.predict(X_test_feat)\n",
    "overlap_acc = accuracy_score(y_test, y_test_pred_ov)\n",
    "\n",
    "print(f\"\\n‚úÖ Overlap RF Accuracy (L4 test): {overlap_acc:.4f}\")\n",
    "print(f\"   vs Original RF: {test_acc:.4f}\")\n",
    "print(f\"   Improvement: {(overlap_acc - test_acc)*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b969d",
   "metadata": {},
   "source": [
    "## üìä FINAL COMPARISON: All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL COMPARISON: All Strategies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_results = pd.DataFrame({\n",
    "    'Strategy': [\n",
    "        'Random Forest (baseline)',\n",
    "        'Random Forest (augmented + extended features)',\n",
    "        'Binary (WORD vs REST)',\n",
    "        'Spectrogram CNN (MobileNetV2)',\n",
    "        'RF with 50% Window Overlap',\n",
    "        'MaxCRNN'\n",
    "    ],\n",
    "    'Test Accuracy (L4)': [\n",
    "        no_aug_acc,\n",
    "        test_acc,\n",
    "        binary_acc,\n",
    "        test_acc_spec,\n",
    "        overlap_acc,\n",
    "        test_acc_nn\n",
    "    ]\n",
    "})\n",
    "\n",
    "final_results = final_results.sort_values('Test Accuracy (L4)', ascending=False)\n",
    "print(final_results.to_string(index=False))\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(final_results)))\n",
    "bars = plt.barh(final_results['Strategy'], final_results['Test Accuracy (L4)'], color=colors)\n",
    "plt.xlabel('Test Accuracy (L4)')\n",
    "plt.title('Accuracy Comparison: All Strategies')\n",
    "plt.xlim(0, 1)\n",
    "for bar, acc in zip(bars, final_results['Test Accuracy (L4)']):\n",
    "    plt.text(acc + 0.01, bar.get_y() + bar.get_height()/2, f'{acc:.2%}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8222340",
   "metadata": {},
   "source": [
    "## üíæ Save All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5aeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "print(\"\\nüíæ Saving all models...\")\n",
    "\n",
    "# RF models\n",
    "with open('rf_augmented.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "with open('rf_binary.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_binary, f)\n",
    "with open('rf_overlap.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_overlap, f)\n",
    "\n",
    "# Neural network models\n",
    "model.save('maxcrnn_phase4.keras')\n",
    "spec_model.save('spectrogram_cnn.keras')\n",
    "\n",
    "# Label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"‚úÖ All models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b21de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Download all results\n",
    "!pip install -q librosa  # Make sure librosa is in requirements\n",
    "!zip -r phase4_all_results.zip *.pkl *.keras *.png\n",
    "\n",
    "files.download('phase4_all_results.zip')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
